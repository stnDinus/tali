@misc{codet5p,
  title = {CodeT5+: Open Code Large Language Models for Code Understanding and
           Generation},
  author = {Yue Wang and Hung Le and Akhilesh Deepak Gotmare and Nghi D. Q. Bui
            and Junnan Li and Steven C. H. Hoi},
  year = {2023},
  eprint = {2305.07922},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/2305.07922},
}
@misc{codet5,
  title = {CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models
           for Code Understanding and Generation},
  author = {Yue Wang and Weishi Wang and Shafiq Joty and Steven C. H. Hoi},
  year = {2021},
  eprint = {2109.00859},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/2109.00859},
}
@misc{codebert,
  title = {CodeBERT: A Pre-Trained Model for Programming and Natural Languages},
  author = {Zhangyin Feng and Daya Guo and Duyu Tang and Nan Duan and Xiaocheng
            Feng and Ming Gong and Linjun Shou and Bing Qin and Ting Liu and
            Daxin Jiang and Ming Zhou},
  year = {2020},
  eprint = {2002.08155},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url = {https://arxiv.org/abs/2002.08155},
}
@misc{gfsa,
  title = {Graph Convolutions Enrich the Self-Attention in Transformers!},
  author = {Jeongwhan Choi and Hyowon Wi and Jayoung Kim and Yehjin Shin and
            Kookjin Lee and Nathaniel Trask and Noseong Park},
  year = {2024},
  eprint = {2312.04234},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/2312.04234},
}
@misc{t5,
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text
           Transformer},
  author = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and
            Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter
            J. Liu},
  year = {2023},
  eprint = {1910.10683},
  archivePrefix = {arXiv},
  primaryClass = {cs.LG},
  url = {https://arxiv.org/abs/1910.10683},
}
